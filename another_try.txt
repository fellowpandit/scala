student@student-HP-Pro-Tower-280-G9-PCI-Desktop-PC:~/Desktop/scala1$ sbt compile
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.jline.terminal.impl.exec.ExecTerminalProvider$ReflectionRedirectPipeCreator (file:/home/student/.sbt/boot/scala-2.12.18/org.scala-sbt/sbt/1.9.9/jline-terminal-3.24.1.jar) to constructor java.lang.ProcessBuilder$RedirectPipeImpl()
WARNING: Please consider reporting this to the maintainers of org.jline.terminal.impl.exec.ExecTerminalProvider$ReflectionRedirectPipeCreator
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[info] Updated file /home/student/Desktop/scala1/project/build.properties: set sbt.version to 1.9.9
[info] welcome to sbt 1.9.9 (Ubuntu Java 11.0.22)
[info] loading project definition from /home/student/Desktop/scala1/project
[info] loading settings for project scala1 from WordCount.sbt ...
[info] set current project to WordCount (in build file:/home/student/Desktop/scala1/)
[info] Executing in batch mode. For better performance use sbt's shell
[info] compiling 1 Scala source to /home/student/Desktop/scala1/target/scala-2.11/classes ...
[success] Total time: 3 s, completed 30-May-2024, 12:33:06 PM
student@student-HP-Pro-Tower-280-G9-PCI-Desktop-PC:~/Desktop/scala1$ sbt run
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.jline.terminal.impl.exec.ExecTerminalProvider$ReflectionRedirectPipeCreator (file:/home/student/.sbt/boot/scala-2.12.18/org.scala-sbt/sbt/1.9.9/jline-terminal-3.24.1.jar) to constructor java.lang.ProcessBuilder$RedirectPipeImpl()
WARNING: Please consider reporting this to the maintainers of org.jline.terminal.impl.exec.ExecTerminalProvider$ReflectionRedirectPipeCreator
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[info] welcome to sbt 1.9.9 (Ubuntu Java 11.0.22)
[info] loading project definition from /home/student/Desktop/scala1/project
[info] loading settings for project scala1 from WordCount.sbt ...
[info] set current project to WordCount (in build file:/home/student/Desktop/scala1/)
[info] running WordCount 
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
24/05/30 12:33:11 WARN Utils: Your hostname, student-HP-Pro-Tower-280-G9-PCI-Desktop-PC resolves to a loopback address: 127.0.1.1; using 172.17.6.30 instead (on interface enp1s0)
24/05/30 12:33:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
24/05/30 12:33:12 INFO SparkContext: Running Spark version 2.3.0
24/05/30 12:33:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/05/30 12:33:12 INFO SparkContext: Submitted application: WordCount
24/05/30 12:33:12 INFO SecurityManager: Changing view acls to: student
24/05/30 12:33:12 INFO SecurityManager: Changing modify acls to: student
24/05/30 12:33:12 INFO SecurityManager: Changing view acls groups to: 
24/05/30 12:33:12 INFO SecurityManager: Changing modify acls groups to: 
24/05/30 12:33:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(student); groups with view permissions: Set(); users  with modify permissions: Set(student); groups with modify permissions: Set()
24/05/30 12:33:12 INFO Utils: Successfully started service 'sparkDriver' on port 38785.
24/05/30 12:33:12 INFO SparkEnv: Registering MapOutputTracker
24/05/30 12:33:12 INFO SparkEnv: Registering BlockManagerMaster
24/05/30 12:33:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/05/30 12:33:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/05/30 12:33:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cb24a3d4-9404-48e7-b1c3-68b2dd4409e2
24/05/30 12:33:12 INFO MemoryStore: MemoryStore started with capacity 434.4 MB
24/05/30 12:33:12 INFO SparkEnv: Registering OutputCommitCoordinator
24/05/30 12:33:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
24/05/30 12:33:12 INFO Utils: Successfully started service 'SparkUI' on port 4041.
24/05/30 12:33:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.17.6.30:4041
24/05/30 12:33:12 INFO Executor: Starting executor ID driver on host localhost
24/05/30 12:33:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33625.
24/05/30 12:33:12 INFO NettyBlockTransferService: Server created on 172.17.6.30:33625
24/05/30 12:33:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/05/30 12:33:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.17.6.30, 33625, None)
24/05/30 12:33:12 INFO BlockManagerMasterEndpoint: Registering block manager 172.17.6.30:33625 with 434.4 MB RAM, BlockManagerId(driver, 172.17.6.30, 33625, None)
24/05/30 12:33:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.17.6.30, 33625, None)
24/05/30 12:33:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.17.6.30, 33625, None)
24/05/30 12:33:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 107.1 KB, free 434.3 MB)
24/05/30 12:33:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 434.3 MB)
24/05/30 12:33:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.6.30:33625 (size: 20.4 KB, free: 434.4 MB)
24/05/30 12:33:12 INFO SparkContext: Created broadcast 0 from textFile at WordCount.scala:9
24/05/30 12:33:12 INFO FileInputFormat: Total input paths to process : 1
24/05/30 12:33:12 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
24/05/30 12:33:12 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
24/05/30 12:33:12 INFO DAGScheduler: Registering RDD 3 (map at WordCount.scala:10)
24/05/30 12:33:12 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions
24/05/30 12:33:12 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:78)
24/05/30 12:33:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
24/05/30 12:33:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
24/05/30 12:33:12 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:10), which has no missing parents
24/05/30 12:33:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 434.3 MB)
24/05/30 12:33:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 434.3 MB)
24/05/30 12:33:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.17.6.30:33625 (size: 2.8 KB, free: 434.4 MB)
24/05/30 12:33:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1039
24/05/30 12:33:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:10) (first 15 tasks are for partitions Vector(0, 1))
24/05/30 12:33:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
24/05/30 12:33:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7877 bytes)
24/05/30 12:33:12 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7877 bytes)
24/05/30 12:33:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/05/30 12:33:12 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
24/05/30 12:33:12 INFO HadoopRDD: Input split: file:/home/student/Desktop/scala1/log.txt:453+453
24/05/30 12:33:12 INFO HadoopRDD: Input split: file:/home/student/Desktop/scala1/log.txt:0+453
24/05/30 12:33:13 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1153 bytes result sent to driver
24/05/30 12:33:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1153 bytes result sent to driver
24/05/30 12:33:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 76 ms on localhost (executor driver) (1/2)
24/05/30 12:33:13 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 69 ms on localhost (executor driver) (2/2)
24/05/30 12:33:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/05/30 12:33:13 INFO DAGScheduler: ShuffleMapStage 0 (map at WordCount.scala:10) finished in 0.114 s
24/05/30 12:33:13 INFO DAGScheduler: looking for newly runnable stages
24/05/30 12:33:13 INFO DAGScheduler: running: Set()
24/05/30 12:33:13 INFO DAGScheduler: waiting: Set(ResultStage 1)
24/05/30 12:33:13 INFO DAGScheduler: failed: Set()
24/05/30 12:33:13 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at saveAsTextFile at WordCount.scala:14), which has no missing parents
24/05/30 12:33:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 65.8 KB, free 434.2 MB)
24/05/30 12:33:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.8 KB, free 434.2 MB)
24/05/30 12:33:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.17.6.30:33625 (size: 23.8 KB, free: 434.4 MB)
24/05/30 12:33:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1039
24/05/30 12:33:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at saveAsTextFile at WordCount.scala:14) (first 15 tasks are for partitions Vector(0, 1))
24/05/30 12:33:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
24/05/30 12:33:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
24/05/30 12:33:13 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7649 bytes)
24/05/30 12:33:13 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
24/05/30 12:33:13 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)
24/05/30 12:33:13 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
24/05/30 12:33:13 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
24/05/30 12:33:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
24/05/30 12:33:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
24/05/30 12:33:13 INFO FileOutputCommitter: Saved output of task 'attempt_20240530123312_0006_m_000000_0' to file:/home/student/Desktop/scala1/wordcountsDir/_temporary/0/task_20240530123312_0006_m_000000
24/05/30 12:33:13 INFO FileOutputCommitter: Saved output of task 'attempt_20240530123312_0006_m_000001_0' to file:/home/student/Desktop/scala1/wordcountsDir/_temporary/0/task_20240530123312_0006_m_000001
24/05/30 12:33:13 INFO SparkHadoopMapRedUtil: attempt_20240530123312_0006_m_000001_0: Committed
24/05/30 12:33:13 INFO SparkHadoopMapRedUtil: attempt_20240530123312_0006_m_000000_0: Committed
24/05/30 12:33:13 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1502 bytes result sent to driver
24/05/30 12:33:13 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1502 bytes result sent to driver
24/05/30 12:33:13 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 54 ms on localhost (executor driver) (1/2)
24/05/30 12:33:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 56 ms on localhost (executor driver) (2/2)
24/05/30 12:33:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/05/30 12:33:13 INFO DAGScheduler: ResultStage 1 (runJob at SparkHadoopWriter.scala:78) finished in 0.067 s
24/05/30 12:33:13 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 0.255775 s
24/05/30 12:33:13 INFO SparkHadoopWriter: Job job_20240530123312_0006 committed.
[success] Total time: 2 s, completed 30-May-2024, 12:33:13 PM
24/05/30 12:33:13 INFO SparkContext: Invoking stop() from shutdown hook
24/05/30 12:33:13 INFO SparkUI: Stopped Spark web UI at http://172.17.6.30:4041
24/05/30 12:33:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/05/30 12:33:13 INFO MemoryStore: MemoryStore cleared
24/05/30 12:33:13 INFO BlockManager: BlockManager stopped
24/05/30 12:33:13 INFO BlockManagerMaster: BlockManagerMaster stopped
24/05/30 12:33:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/05/30 12:33:13 INFO SparkContext: Successfully stopped SparkContext
24/05/30 12:33:13 INFO ShutdownHookManager: Shutdown hook called
24/05/30 12:33:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-e7d382e7-3994-4c1d-9b5b-032971952c65
student@student-HP-Pro-Tower-280-G9-PCI-Desktop-PC:~/Desktop/scala1$ sbt package
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.jline.terminal.impl.exec.ExecTerminalProvider$ReflectionRedirectPipeCreator (file:/home/student/.sbt/boot/scala-2.12.18/org.scala-sbt/sbt/1.9.9/jline-terminal-3.24.1.jar) to constructor java.lang.ProcessBuilder$RedirectPipeImpl()
WARNING: Please consider reporting this to the maintainers of org.jline.terminal.impl.exec.ExecTerminalProvider$ReflectionRedirectPipeCreator
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[info] welcome to sbt 1.9.9 (Ubuntu Java 11.0.22)
[info] loading project definition from /home/student/Desktop/scala1/project
[info] loading settings for project scala1 from WordCount.sbt ...
[info] set current project to WordCount (in build file:/home/student/Desktop/scala1/)
[success] Total time: 1 s, completed 30-May-2024, 12:33:18 PM






scala> val data = Seq((1,"a"),(2,"b"),(3,"c"),(4,"d"))
data: Seq[(Int, String)] = List((1,a), (2,b), (3,c), (4,d))

scala> val rdd = sc.parallelize(data)
rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> import org.apache.spark.RangePartitioner
import org.apache.spark.RangePartitioner

scala> val partitionedRDD = rdd. partitionBy(new RangePartitioner(2,rdd))
partitionedRDD: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[3] at partitionBy at <console>:24

scala> partitionedRDD.mapPartitionsWithIndex((index,iterator)=>iterator.map(x=>(index,x))).collect().foreach(println)
(0,(1,a))
(0,(2,b))
(1,(3,c))
(1,(4,d))

scala> import org.apache.spark.HashPartitioner
import org.apache.spark.HashPartitioner

scala> val parditionedRDD = rdd.partitionBy(new HashPartitioner(4))
parditionedRDD: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[5] at partitionBy at <console>:25

scala> partitionedRDD.mapPartitionsWithIndex {(index, iterator)=>iterator.map(x=>(index,x))}.collect().foreach(println)
(0,(1,a))
(0,(2,b))
(1,(3,c))
(1,(4,d))
